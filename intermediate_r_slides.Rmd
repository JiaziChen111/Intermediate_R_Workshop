---
title: "Introduction to R"
subtitle: "UW Tacoma"  
author: "Charles Lanfear"
date: "Jan 31, 2019<br>Updated: `r gsub(' 0', ' ', format(Sys.Date(), format='%b %d, %Y'))`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: tomorrow-night-bright
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["center","top"]
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE}
library(xaringanthemer)

csss508css <- list(
     ".hljs-tomorrow-night-bright .hljs" = list(
        "background" = "#10102c",
        "border-radius"="5px"),
     ".remark-inline-code" = list(
        "background" = "#E0E0E0",
        "color" = "#10102c",
        "border-radius" = "3px",
        "padding" = "2px"),
     ".inverse .remark-inline-code" = list(
        "background" = "#10102c",
        "color" = "#ececf8",
        "border-radius" = "3px",
        "padding" = "2px"),
     ".small" = list("font-size" = "75%"),
     ".smaller" = list("font-size" = "60%"),
     ".remark-code-line-highlighted" = list(
        "background-color" = "rgba(255,0,255,0.2)"),
     "sup" = list("font-size" = "14px"),
     "h1, h2, h3" = list(
        "margin-top"=".25em", 
        "margin-bottom"=".25em"),
     ".pull-left60" = list(
        "float" = "left",
        "width" = "58%" ),
     ".pull-right40" = list(
        "float" = "right",
        "width" = "38%" ),
     ".pull-right30" = list(
        "float" = "right",
        "width" = "30%" ),
     "a" = list("text-decoration" = "underline"),
     ".inverse a" = list("color" = "#cbd3a3"),
     "body" = list("line-height" = "1.4"),
     ".inverse" = list("background-image" = "url(https://clanfear.github.io/Intro_R_Workshop/img/UWTacomaBG1_white.svg)")
    )

mono_accent(base_color = "#342c5c",
            code_font_google = google_font("Fira Mono"),
            header_font_google = google_font("Quattrocento"),
            extra_css = csss508css,
            title_slide_background_image = "https://clanfear.github.io/Intermediate_R_Workshop/img/title_slide_img.png",
            title_slide_background_position = "bottom",
            title_slide_background_size = "contain",
            background_image = "https://clanfear.github.io/Intermediate_R_Workshop/img/UWTacomaBG1.svg"
)
```


# Overview

1. Subsetting Data

2. Creating Variables and Summarizing Data

3. Joining Data

4. Reshaping: Wide and Long Data

5. Resources for Further Learning

---
class: inverse

# Modifying Data Frames with `dplyr`


---
# But First, Pipes (%>%)

`dplyr` uses the [`magrittr`](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) forward pipe operator, usually called simply a **pipe**. We write pipes like `%>%` (`Ctrl+Shift+M`).

--

Pipes take the object on the *left* and apply the function on the *right*: `x %>% f(y) = f(x, y)`. Read out loud: "and then..."

--

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(gapminder)
gapminder %>% filter(country == "Canada") %>% head(2)
```

--

Pipes save us typing, make code readable, and allow chaining like above, so we use them *all the time* when manipulating data frames.

---

# Using Pipes


Pipes are clearer to read when you have each function on a separate line (inconsistent in these slides because of space constraints).

--

```{r, eval=FALSE}
take_this_data %>%
    do_first_thing(with = this_value) %>%
    do_next_thing(using = that_value) %>% ...
```

--

Stuff to the left of the pipe is passed to the *first argument* of the function on the right. Other arguments go on the right in the function. 

--

If you ever find yourself piping a function where data are not the first argument, use `.` in the data argument instead.
```{r, eval=FALSE}
yugoslavia %>% lm(pop ~ year, data = .)
```

---
# Filtering Rows (subsetting)

Recall last week we used the `filter()` command to subset data like so:
```{r}
Canada <- gapminder %>%
    filter(country == "Canada")
```

Excel analogue: Filter!

![Excel's filter](http://content.gcflearnfree.org/topics/143/ex07_filter.gif)

---
# Another Operator: `%in%`

Common use case: Filter rows to things in some *set*.

We can use `%in%` like `==` but for matching *any element* in the vector on its right<sup>1</sup>. 

```{r}
former_yugoslavia <- c("Bosnia and Herzegovina", "Croatia", 
              "Macedonia", "Montenegro", "Serbia", "Slovenia")
yugoslavia <- gapminder %>% filter(country %in% former_yugoslavia)
tail(yugoslavia, 2)
```

.footnote[[1] The `c()` function is how we make **vectors** in R, which are an important data type.]
---
## Sorting: `arrange()`

Along with filtering the data to see certain rows, we might want to sort it:

```{r}
yugoslavia %>% arrange(year, desc(pop))
```

The data are sorted by ascending `year` and descending `pop`.

---
## Keeping Columns: `select()`

Not only can we limit rows, but we can include specific columns (and put them in the order listed) using `select()`. 

```{r}
yugoslavia %>% select(country, year, pop) %>% head(4)
```

---
## Dropping Columns: `select()`


We can instead drop only specific columns with `select()` using `-` signs:

```{r}
yugoslavia %>% select(-continent, -pop, -lifeExp) %>% head(4)
```

---
## Helper Functions for `select()`


`select()` has a variety of helper functions like `starts_with()`, `ends_with()`, and `contains()`, or can be given a range of continguous columns `startvar:endvar`. See `?select` for details.

These are very useful if you have a "wide" data frame with column names following a pattern or ordering. 

![DYS Data Example](http://clanfear.github.io/CSSS508/Lectures/Week3/img/dys_vars.PNG)

```{r, eval=FALSE}
DYS %>% select(starts_with("married"))
DYS %>% select(ends_with("18"))
```

---
## Renaming Columns with `select()`


We can rename columns using `select()`, but that drops everything that isn't mentioned:

```{r}
yugoslavia %>%
    select(Life_Expectancy = lifeExp) %>%
    head(4)
```

---
### Safer: Rename Columns with `rename()`


`rename()` renames variables using the same syntax as `select()` without dropping unmentioned variables.

```{r}
yugoslavia %>%
    select(country, year, lifeExp) %>%
    rename(Life_Expectancy = lifeExp) %>%
    head(4)
```

---
## Column Naming Practices

* *Good* column names will be self-describing. Don't use inscrutable abbreviations to save typing. RStudio's autocompleting functions take away the pain of long variable names: Hit `TAB` while writing code to autocomplete.

--

* *Valid* "naked" column names can contain upper or lowercase letters, numbers, periods, and underscores. They must start with a letter or period and not be a special reserved word (e.g. `TRUE`, `if`).

--

* Names are case-sensitive: `Year` and `year` are not the same thing!

--

* You can include spaces or use reserved words if you put backticks around the name. Spaces can be worth including when preparing data for `ggplot2` or `pander` since you don't have to rename axes or table headings.

---

## Column Name with Space Example

```{r}
library(pander)
yugoslavia %>% filter(country == "Serbia") %>%
    select(year, lifeExp) %>%
    rename(Year = year, `Life Expectancy` = lifeExp) %>%
    head(5) %>%
    pander(style = "rmarkdown", caption = "Serbian life expectancy")
```

---
## Create New Columns: `mutate()`

In `dplyr`, you can add new columns to a data frame using `mutate()`.

--


```{r}
yugoslavia %>% filter(country == "Serbia") %>%
    select(year, pop, lifeExp) %>%
    mutate(pop_million = pop / 1000000,
           life_exp_past_40 = lifeExp - 40) %>%
    head(5)
```

Note you can create multiple variables in a single `mutate()` call by separating the expressions with commas.

---
# `ifelse()`


A common function used in `mutate()` (and in general in R programming) is `ifelse()`. It returns a vector of values depending on a logical test.

```{r, eval=FALSE}
ifelse(test = x==y, yes = first_value , no = second_value)
```

Output from `ifelse()` if `x==y` is...
* `TRUE`: `first_value` - the value for `yes =`

* `FALSE`: `second_value` - the value for `no = `

* `NA`: `NA` - because you can't test for NA with an equality!

--

For example:

```{r}
example <- c(1, 0, NA, -2)
ifelse(example > 0, "Positive", "Not Positive")
```

---
# `ifelse()` Example


```{r}
yugoslavia %>% mutate(short_country = 
               ifelse(country == "Bosnia and Herzegovina", 
                    "B and H", as.character(country))) %>%
    select(short_country, year, pop) %>%
    arrange(year, short_country) %>%
    head(3)
```

Read this as "For each row, if country equals 'Bosnia and Herzegovina, make `short_country` equal to 'B and H', otherwise make it equal to that row's value of `country`."

This is a simple way to change some values but not others!

---
# `recode()`


`recode()` is another useful function to use inside `mutate()`. Use `recode()` to change specific values to other values, particularly with factors. You can change multiple values at the same time. Note if a value has spaces in it, you'll need to put it in backticks!

```{r}
yugoslavia %>% 
  mutate(country = recode(country, 
                        `Bosnia and Herzegovina`="B and H", #<<
                        Montenegro="M")) %>% 
  distinct(country)
```

---
# `case_when()`

`case_when()` performs multiple `ifelse()` operations at the same time. `case_when()` allows you to create a new variable with values based on multiple logical statements. This is useful for making categorical variables or variables from combinations of other variables.
.small[
```{r}
gapminder %>% mutate(gdpPercap_ordinal = case_when(
    gdpPercap < 700 ~ "low",
    gdpPercap >= 700 & gdpPercap < 800 ~ "moderate",
    TRUE ~ "high" )) # Value when all other statements are FALSE
```
]

---
class: inverse

# Summarizing with `dplyr`

---
## General Aggregation: `summarize()`

`summarize()` takes your rows of data and computes something across them: count how many rows there are, calculate the mean or total, etc. You can use any function that aggregates *multiple values* into a *single value* (like `sd()`).

In a spreadsheet:

![Excel equivalent of summing a column](https://osiprodeusodcspstoa01.blob.core.windows.net/en-us/media/5feb1ba8-a0fb-49d1-8188-dcf1ba878a42.jpg)

---
# `summarize()` Example

```{r}
yugoslavia %>%
    filter(year == 1982) %>%
    summarize(n_obs = n(),
              total_pop = sum(pop),
              mean_life_exp = mean(lifeExp),
              range_life_exp = max(lifeExp) - min(lifeExp))
```

These new variables are calculated using *all of the rows* in `yugoslavia`

---
# Avoiding Repetition: 

### `summarize_at()`


Maybe you need to calculate the mean and standard deviation of a bunch of columns. With `summarize_at()`, put the variables to compute over first in `vars()` (like `select()` syntax) and put the functions to use in a `funs()` after.

```{r}
yugoslavia %>%
    filter(year == 1982) %>%
    summarize_at(vars(lifeExp, pop), funs(mean, sd))
```

Note it automatically names the summarized variables based on the functions used to summarize.

---
# Avoiding Repetition

### Other functions:


There are additional `dplyr` functions similar to `summarize_at()`:

* `summarize_all()` and `mutate_all()` summarize / mutate *all* variables sent to them in the same way. For instance, getting the mean and standard deviation of an entire dataframe:

```{r, eval=FALSE}
dataframe %>% summarize_all(funs(mean, sd))
```

* `summarize_if()` and `mutate_if()` summarize / mutate all variables that satisfy some logical condition. For instance, summarizing every numeric column in a dataframe at once:

```{r, eval=FALSE}
dataframe %>% summarize_if(is.numeric, funs(mean, sd))
```

You can use all of these to avoid typing out the same code repeatedly!

---
# `group_by()`


The special function `group_by()` changes how functions operate on the data, most importantly `summarize()`.

Functions after `group_by()` are computed *within each group* as defined by variables given, rather than over all rows at once. Typically the variables you group by will be integers, factors, or characters, and not continuous real values.

Excel analogue: pivot tables

.image-50[![Pivot table](http://www.excel-easy.com/data-analysis/images/pivot-tables/two-dimensional-pivot-table.png)]

---
`group_by()` example


```{r}
yugoslavia %>%
  group_by(year) %>% #<<
    summarize(num_countries = n_distinct(country),
              total_pop = sum(pop),
              total_gdp_per_cap = sum(pop*gdpPercap)/total_pop) %>%
    head(5)
```

Because we did `group_by()` with `year` then used `summarize()`, we get *one row per value of `year`*!

---
## Window Functions

Grouping can also be used with `mutate()` or `filter()` to give rank orders within a group, lagged values, and cumulative sums. You can read more about window functions in this [vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html).

```{r}
yugoslavia %>% 
  select(country, year, pop) %>%
  filter(year >= 2002) %>% 
  group_by(country) %>%
  mutate(lag_pop = lag(pop, order_by = year),
         pop_chg = pop - lag_pop) %>%
  head(4)
```

---
class: inverse
##Joining (Merging) Data Frames

---
## When Do We Need to Join Tables?

* Want to make columns using criteria too complicated for `ifelse()` or `case_when()`
* Combine data stored in separate places: e.g. UW registrar information with student homework grades

Excel equivalents: `VLOOKUP`, `MATCH`

.image-75[
![VLOOKUP example](https://cdn.ablebits.com/_img-blog/excel-vlookup/excel-vlookup.png)
]

---
## Joining in Concept

We need to think about the following when we want to merge data frames `A` and `B`:

* Which *rows* are we keeping from each data frame?

* Which *columns* are we keeping from each data frame?

* Which variables determine whether rows *match*?

---
## Join Types: Rows and columns kept

There are many types of joins<sup>1</sup>...

* `A %>% left_join(B)`: keep all rows from `A`, matched with `B` wherever possible (`NA` when not), keep columns from both `A` and `B`

* `A %>% right_join(B)`: keep all rows from `B`, matched with `A` wherever possible (`NA` when not), keep columns from both `A` and `B`

* `A %>% inner_join(B)`: keep rows from `A` that match rows in `B`, keep columns from both `A` and `B`

* `A %>% full_join(B)`: keep all rows from either `A` or `B`, matched wherever possible (`NA` when not), keep columns from both `A` and `B`

* `A %>% semi_join(B)`: keep rows from `A` that match rows in `B`, keep columns from only `A`

* `A %>% anti_join(B)`: keep rows from `A` that *don't* match a row in `B`, keep columns from only `A`

.pull-right[.footnote[[1] Usually `left_join()` does the job.]]

---
## Matching Criteria

We say rows should *match* because they have some columns containing the same value. We list these in a `by = ` argument to the join.

Matching Behavior:

* No `by`: Match using all variables in `A` and `B` that have identical names

--

* `by = c("var1", "var2", "var3")`: Match on identical values of `var1`, `var2`, and `var3` in both `A` and `B`

--

* `by = c("Avar1" = "Bvar1", "Avar2" = "Bvar2")`: Match identical values of `Avar1` variable in `A` to `Bvar1` variable in `B`, and `Avar2` variable in `A` to `Bvar2` variable in `B`

Note: If there are multiple matches, you'll get *one row for each possible combination* (except with `semi_join()` and `anti_join()`).

Need to get more complicated? Break it into multiple operations.

---
## `nycflights13` Data

We'll use data in the [`nycflights13` package](https://cran.r-project.org/web/packages/nycflights13/nycflights13.pdf). Install and load it:
```{r}
# install.packages("nycflights13") # Uncomment to run
library(nycflights13)
```

It includes five dataframes, some of which contain missing data (`NA`):

* `flights`: flights leaving JFK, LGA, or EWR in 2013
* `airlines`: airline abbreviations
* `airports`: airport metadata
* `planes`: airplane metadata
* `weather`: hourly weather data for JFK, LGA, and EWR

Note these are *separate data frames*, each needing to be *loaded separately*:

```{r, eval=FALSE}
data(flights)
data(airlines)
data(airports)
# and so on...
```

---
## Join Example #1

Who manufactures the planes that flew to Seattle?
```{r}
flights %>% filter(dest == "SEA") %>% select(tailnum) %>%
    left_join(planes %>% select(tailnum, manufacturer), #<<
              by = "tailnum") %>%
    count(manufacturer) %>% # Count observations of by manufacturer
    arrange(desc(n)) # Arrange data descending by count
```

Note you can perform operations on the data inside functions such as `left_join()` and the *result* will be used by the function.

---
## Join Example #2

Which airlines had the most flights to Seattle from NYC?
```{r}
flights %>% filter(dest == "SEA") %>% 
    select(carrier) %>%
    left_join(airlines, by = "carrier") %>%
    group_by(name) %>% 
    tally() %>% #<<
    arrange(desc(n))
```

`tally()` is a shortcut for `summarize(n(.))`: It creates a variable `n` equal to the number of rows in each group.

---
# Slightly Messy Data

| **Program**     | **Female** | **Male** |
|-----------------|-----------:|---------:|
| Evans School    |     10     |    6    |
| Arts & Sciences |      5     |    6    |
| Public Health   |      2     |    3    |
| Other           |      5     |    1    |

--

* What is an observation?
    + A group of students from a program of a given gender
* What are the variables?
    + Program, gender
* What are the values?
    + Program: Evans School, Arts & Sciences, Public Health, Other
    + Gender: Female, Male -- **in the column headings, not its own column!**
    + Count: **spread over two columns!**

---
# Tidy Version

| **Program**     | **Gender** | **Count** |
|-----------------|-----------:|---------:|
| Evans School    |     Female |    10   |
| Evans School    |     Male   |    6    |
| Arts & Sciences |     Female |    5    |
| Arts & Sciences |     Male   |    6    |
| Public Health   |     Female |    2    |
| Public Health   |     Male   |    3    |
| Other           |     Female |    5    |
| Other           |     Male   |    1    |

Each variable is a column.

Each observation is a row.

Ready to throw into `ggplot()`!

---
# Billboard is Just Ugly-Messy

.small[
```{r, echo=FALSE}
library(pander)
pander(head(billboard_2000_raw[,1:10], 12), split.tables=120, style="rmarkdown")
```
]

Week columns continue up to `wk76`!

---
# Billboard

* What are the **observations** in the data?

--

    + Week since entering the Billboard Hot 100 per song
--

* What are the **variables** in the data?
--

    + Year, artist, track, song length, date entered Hot 100, week since first entered Hot 100 (**spread over many columns**), rank during week (**spread over many columns**)
--

* What are the **values** in the data?
--

    + e.g. 2000; 3 Doors Down; Kryptonite; 3 minutes 53 seconds; April 8, 2000; Week 3 (**stuck in column headings**); rank 68 (**spread over many columns**)

---
# Tidy Data

**Tidy data** (aka "long data") are such that:

--

1. The values for a single observation are in their own row.
--

2. The values for a single variable are in their own column.
--

3. The observations are all of the same nature.

--

Why do we want tidy data?

* Easier to understand many rows than many columns
* Required for plotting in `ggplot2`
* Required for many types of statistical procedures (e.g. hierarchical or mixed effects models)
* Fewer confusing variable names
* Fewer issues with missing values and "imbalanced" repeated measures data

---
# tidyr

The `tidyr` package provides functions to tidy up data, similar to `reshape` in Stata or `varstocases` in SPSS. Key functions:

--

* `gather()`: takes a set of columns and rotates them down to make two new columns (which you can name yourself): 
    * A `key` that stores the original column names
    * A `value` with the values in those original columns

--

* `spread()`: inverts `gather()` by taking two columns and rotating them up into multiple columns

--

* `separate()`: pulls apart one column into multiple columns (common with `gather`ed data where values had been embedded in column names)
    * `extract_numeric()` does a simple version of this for the common case when you just want grab the number part

--

* `unite()`: inverts `separate()` by gluing together multiple columns into one character column (less common)

---
# `gather()`

Let's use `gather()` to get the week and rank variables out of their current layout into two columns (big increase in rows, big drop in columns):

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
billboard_2000 <- billboard_2000_raw %>%
    gather(key = week, value = rank, starts_with("wk")) #<<
dim(billboard_2000)
```

`starts_with()` and other helper functions from `dplyr::select()` work here too.

We could instead use: `gather(key = week, value = rank, wk1:wk76)` to pull out these contiguous columns.

---
# `gather`ed Weeks

.smallish[
```{r, message=FALSE, warning=FALSE}
head(billboard_2000)
```
]

Now we have a single week column!

---
# Gathering Better?

```{r}
summary(billboard_2000$rank)
```

This is an improvement, but we don't want to keep the `r sum(is.na(billboard_2000$rank))` rows with missing ranks (i.e. observations for weeks since entering the Hot 100 that the song was no longer on the Hot 100).

---
# Gathering Better: `na.rm`

The argument `na.rm = TRUE` to `gather()` will remove rows with missing ranks.
```{r}
billboard_2000 <- billboard_2000_raw %>%
    gather(key = week, value = rank, starts_with("wk"),
           na.rm = TRUE) #<<
summary(billboard_2000$rank)
```

---
# `separate()`

The track length column isn't analytically friendly. Let's convert it to a number rather than the character (minutes:seconds) format:

```{r}
billboard_2000 <- billboard_2000 %>%
    separate(time, into = c("minutes", "seconds"),
             sep = ":", convert = TRUE) %>% #<<
    mutate(length = minutes + seconds / 60) %>%
    select(-minutes, -seconds)
summary(billboard_2000$length)
```

`sep = :` tells `separate()` to split the column into two where it finds a colon (`:`).

Then we add `seconds / 60` to `minutes` to produce a numeric `length` in minutes.

---
# `spread()` Motivation

`spread()` is the opposite of `gather()`, which you use if you have data for the same observation taking up multiple rows.

--

Example of data that we probably want to spread (unless we want to plot each statistic in its own facet):

.small[
| **Group** | **Statistic** | **Value** |
|-------|-----------|------:|
| A     | Mean      |  1.28 |
| A     | Median    |   1.0 |
| A     | SD        |  0.72 |
| B     | Mean      |  2.81 |
| B     | Median    |     2 |
| B     | SD        |  1.33 |
]

A common cue to use `spread()` is having measurements of different quantities in the same column. 

---
# Before `spread()`

.smallish[
```{r}
(too_long_data <- data.frame(Group = c(rep("A", 3), rep("B", 3)),
                             Statistic = rep(c("Mean", "Median", "SD"), 2),
                             Value = c(1.28, 1.0, 0.72, 2.81, 2, 1.33)))
```
]

---
# After `spread()`

```{r}
(just_right_data <- too_long_data %>%
    spread(key = Statistic, value = Value))
```

---
# Charts of 2000: Data Prep

Let's look at songs that hit #1 at some point and look how they got there versus songs that did not:

```{r}
# find best rank for each song
best_rank <- billboard_2000 %>%
    group_by(artist, track) %>%
    summarize(min_rank = min(rank), #<<
              weeks_at_1 = sum(rank == 1)) %>%
    mutate(`Peak rank` = ifelse(min_rank == 1,
                                "Hit #1",
                                "Didn't #1"))

# merge onto original data
billboard_2000 <- billboard_2000 %>%
    left_join(best_rank, by = c("artist", "track"))
```

.footnote[Note that because the "highest" rank is *numerically lowest* (1), we are summarizing with `min()`.]

---
# Charts of 2000: `ggplot2`

```{r}
library(ggplot2)
billboard_trajectories <- 
  ggplot(data = billboard_2000,
         aes(x = week, y = rank, group = track,
             color = `Peak rank`)
         ) +
  geom_line(aes(size = `Peak rank`), alpha = 0.4) +
    # rescale time: early weeks more important
  scale_x_log10(breaks = seq(0, 70, 10)) + 
  scale_y_reverse() + # want rank 1 on top, not bottom
  theme_classic() +
  xlab("Week") + ylab("Rank") +
  scale_color_manual(values = c("black", "red")) +
  scale_size_manual(values = c(0.25, 1)) +
  theme(legend.position = c(0.90, 0.25),
        legend.background = element_rect(fill="transparent"))
```

---
# Charts of 2000: Beauty!

```{r, cache=FALSE, echo=FALSE, dev="svg", fig.height=4}
billboard_trajectories
```

Observation: There appears to be censoring around week 20 for songs falling out of the top 50 that I'd want to follow up on.

---
## Which Were #1 the Most Weeks?

```{r}
billboard_2000 %>%
    select(artist, track, weeks_at_1) %>%
    distinct(artist, track, weeks_at_1) %>%
    arrange(desc(weeks_at_1)) %>%
    head(7)
```

---
class: inverse
# Dates and Times

---
# Getting Usable Dates

We have the date the songs first charted, but not the dates for later weeks. We can calculate these now that the data are tidy:

```{r}
billboard_2000 <- billboard_2000 %>%
    mutate(date = date.entered + (week - 1) * 7) #<<
billboard_2000 %>% arrange(artist, track, week) %>%
    select(artist, date.entered, week, date, rank) %>% head(4)
```

This works because `date` objects are in units of days—we just add 7 days per week to the start date.

---
# Preparing to Plot Over Calendar Time

.smallish[
```{r}
plot_by_day <- 
  ggplot(billboard_2000, aes(x = date, y = rank, group = track)) +
  geom_line(size = 0.25, alpha = 0.4) +
  # just show the month abbreviation label (%b)
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  scale_y_reverse() + theme_bw() +
  # add lines for start and end of year:
  # input as dates, then make numeric for plotting
  geom_vline(xintercept = as.numeric(as.Date("2000-01-01", "%Y-%m-%d")),
             col = "red") +
  geom_vline(xintercept = as.numeric(as.Date("2000-12-31", "%Y-%m-%d")),
             col = "red") +
  xlab("Week") + ylab("Rank")
```
]

---
# Calendar Time Plot!

```{r, echo=FALSE, dev="svg", fig.height=4}
plot_by_day
```

We see some of the entry dates are before 2000---presumably songs still charting during 2000 that came out earlier. 

---
# Dates and Times

To practice working with finer-grained temporal information, let's look at one day of Seattle Police response data obtained from [data.seattle.gov](http://data.seattle.gov):

.smaller[
```{r, cache=TRUE}
spd_raw <- read_csv("https://raw.githubusercontent.com/clanfear/CSSS508/master/Seattle_Police_Department_911_Incident_Response.csv")
```
]

---
# Resources

   * [UW CSSS508](https://clanfear.github.io/CSSS508/): My University of Washington Introduction to R course which forms the basis for this workshop. All content including lecture videos is freely available.
   * [R for Data Science](http://r4ds.had.co.nz/) online textbook by Garrett Grolemund and Hadley Wickham. One of many good R texts available, but importantly it is free and focuses on the [`tidyverse`](http://tidyverse.org/) collection of R packages which are the modern standard for data manipulation and visualization in R.
   * [Advanced R](http://adv-r.had.co.nz/) online textbook by Hadley Wickham. A great source for more in-depth and advanced R programming.
   * [DataCamp](https://www.datacamp.com/): A source for interactive R tutorials (some free of charge).
   * [`swirl`](http://swirlstats.com/students.html): Interactive tutorials inside R.
   * [Useful RStudio cheatsheets](https://www.rstudio.com/resources/cheatsheets/) on R Markdown, RStudio shortcuts, etc.
